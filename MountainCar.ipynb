{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hahamark/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(1); torch.manual_seed(1); np.random.seed(1)\n",
    "PATH = glob.glob(os.path.expanduser('~/tboardlogs/'))\n",
    "writer = SummaryWriter('{}{}'.format(PATH, datetime.now().strftime('%b%d_%H-%M-%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        # YOUR CODE HERE\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # YOUR CODE HERE\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-0.43852191,  0.        ]), 0, -1.0, array([-0.44015308, -0.00163117]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Furthest Position: -0.1585655553751691\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful episodes: 0\n"
     ]
    }
   ],
   "source": [
    "max_position = -.4\n",
    "positions = np.ndarray([0,2])\n",
    "rewards = []\n",
    "successful = []\n",
    "for episode in range(1000):\n",
    "    running_reward = 0\n",
    "    env.reset()\n",
    "    done = False\n",
    "    for i in range(200):\n",
    "        state, reward, done, _ = env.step(np.random.randint(0,3))\n",
    "        # Give a reward for reaching a new maximum position\n",
    "        if state[0] > max_position:\n",
    "            max_position = state[0]\n",
    "            positions = np.append(positions, [[episode, max_position]], axis=0)\n",
    "            running_reward += 10\n",
    "        else:\n",
    "            running_reward += reward\n",
    "        if done: \n",
    "            if state[0] >= 0.5:\n",
    "                successful.append(episode)\n",
    "            rewards.append(running_reward)\n",
    "            break\n",
    "\n",
    "print('Furthest Position: {}'.format(max_position))\n",
    "plt.figure(1, figsize=[10,5])\n",
    "plt.subplot(211)\n",
    "plt.plot(positions[:,0], positions[:,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Furthest Position')\n",
    "plt.subplot(212)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "print('successful episodes: {}'.format(np.count_nonzero(successful)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried different weight initializations but found they did not perform well.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight, 0, 1)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.fc1 = nn.Linear(self.state_space, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, self.action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN(policy, loss_fn, memory, optimizer, batch_size, gamma):\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, state_1, done = zip(*transitions)\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    \n",
    "    state_1 = torch.tensor(state_1, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    Q = policy(Variable(state))\n",
    "    Q1 = policy(Variable(state_1))\n",
    "    \n",
    "    maxQ1, _ = torch.max(Q1, -1)\n",
    "\n",
    "    # Create target Q value for training the policy\n",
    "    Q_target = Q.clone()\n",
    "    Q_target = Variable(Q_target)\n",
    "    for a in action:\n",
    "        Q_target[a, action[a]] = reward[a] + torch.mul(maxQ1[a].detach(), gamma)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(Q, Q_target)\n",
    "\n",
    "    # Update policy\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()               \n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDDQN(policy1, policy2, loss_fn, memory, optimizer, batch_size, gamma):\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, state_1, done = zip(*transitions)\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    \n",
    "    state_1 = torch.tensor(state_1, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # Find max Q for t+1 state\n",
    "    if turn1:\n",
    "        Q1 = policy2(Variable(state_1)).detach()\n",
    "        Q2 = policy1(Variable(state_1))\n",
    "    else:\n",
    "        Q1 = policy1(Variable(state_1)).detach()\n",
    "        Q2 = policy2(Variable(state_1))\n",
    "\n",
    "    _, a_prime = torch.max(Q1, -1)\n",
    "\n",
    "    maxQ1 = Q2[a_prime]\n",
    "\n",
    "    # Create target Q value for training the policy\n",
    "    Q_target = Q2.clone()\n",
    "    Q_target = Variable(Q_target)\n",
    "    for a in action:\n",
    "        Q_target[a, action[a]] = reward[a] + torch.mul(maxQ1[a], gamma)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(Q1, Q_target)\n",
    "\n",
    "    # Update policy\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(episodes):\n",
    "    \n",
    "    # Parameters\n",
    "    steps = 200\n",
    "    state = env.reset()\n",
    "    epsilon = 0.3\n",
    "    gamma = 0.99\n",
    "    loss_history = []\n",
    "    reward_history = []\n",
    "    learning_rate = 0.001\n",
    "    successes = 0\n",
    "    position = []\n",
    "    memory = ReplayMemory(10000)\n",
    "    batch_size = 32\n",
    "\n",
    "    # Initialize Policy\n",
    "    policy = Policy()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(policy.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "    for episode in tqdm_notebook(range(episodes)):\n",
    "        episode_loss = 0\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for s in range(steps):\n",
    "            # Get first action value function\n",
    "            Q = policy(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "\n",
    "            # Choose epsilon-greedy action\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                action = np.random.randint(0,3)\n",
    "            else:\n",
    "                _, action = torch.max(Q, -1)\n",
    "                action = action.item()\n",
    "\n",
    "            # Step forward and receive next state and reward\n",
    "            state_1, reward, done, _ = env.step(action)\n",
    "            klaar = done\n",
    "\n",
    "            # Adjust reward based on car position\n",
    "            reward = state_1[0] + 0.5\n",
    "\n",
    "            # Adjust reward for task completion\n",
    "            if state_1[0] >= 0.5:\n",
    "                reward += 1\n",
    "\n",
    "            # Train the DQN\n",
    "            memory.push((state, action, reward, state_1, done))\n",
    "            loss = trainDQN(policy, loss_fn, memory, optimizer, batch_size, gamma)\n",
    "\n",
    "            if done:\n",
    "                if state_1[0] >= 0.5:\n",
    "                    # On successful epsisodes, adjust the following parameters\n",
    "                    # Adjust epsilon\n",
    "                    epsilon *= .95\n",
    "                    # Adjust learning rate\n",
    "                    scheduler.step()\n",
    "                    # Record successful episode\n",
    "                    successes += 1\n",
    "\n",
    "                # Record history\n",
    "                loss_history.append(episode_loss)\n",
    "                reward_history.append(episode_reward)\n",
    "                position.append(state_1[0])\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                state = state_1\n",
    "                \n",
    "    print('successful episodes: {:d} - {:.4f}%'.format(successes, successes/episodes*100))\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleDQN(episodes):\n",
    "    \n",
    "    # Parameters\n",
    "    steps = 200\n",
    "    state = env.reset()\n",
    "    epsilon = 0.3\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    loss_history = []\n",
    "    reward_history = []\n",
    "    learning_rate = 0.001\n",
    "    successes = 0\n",
    "    position = []\n",
    "\n",
    "    # Initialize Policy\n",
    "    policy1 = Policy()\n",
    "    policy2 = Policy()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD([\n",
    "                {'params': policy1.parameters(), 'lr': learning_rate},\n",
    "                {'params': policy2.parameters(), 'lr': learning_rate}\n",
    "                ])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "    for episode in tqdm_notebook(range(episodes)):\n",
    "        episode_loss = 0\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for s in range(steps):\n",
    "            # Get first action value function\n",
    "            turn1 = random.choice([True, False])\n",
    "            if turn1:\n",
    "                Q = policy1(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "            else:\n",
    "                Q = policy2(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
    "                \n",
    "            # Choose epsilon-greedy action\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                action = np.random.randint(0,3)\n",
    "            else:\n",
    "                _, action = torch.max(Q, -1)\n",
    "                action = action.item()\n",
    "\n",
    "            # Step forward and receive next state and reward\n",
    "            state_1, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Adjust reward based on car position\n",
    "            reward = state_1[0] + 0.5\n",
    "            # Adjust reward for task completion\n",
    "            if state_1[0] >= 0.5:\n",
    "                reward += 1\n",
    "                \n",
    "            # Train the DQN\n",
    "            memory.push((state, action, reward, state_1, done))\n",
    "            loss = trainDDQN(policy1, policy2, loss_fn, memory, optimizer, batch_size, gamma)\n",
    "            if loss:\n",
    "                \n",
    "                episode_loss += loss.item()\n",
    "                episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                if state_1[0] >= 0.5:\n",
    "                    # On successful epsisodes, adjust the following parameters\n",
    "                    # Adjust epsilon\n",
    "                    epsilon *= .95\n",
    "\n",
    "                    # Adjust learning rate\n",
    "                    scheduler.step()\n",
    "\n",
    "                    # Record successful episode\n",
    "                    successes += 1\n",
    "\n",
    "                # Record history\n",
    "                loss_history.append(episode_loss)\n",
    "                reward_history.append(episode_reward)\n",
    "                position.append(state_1[0])\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                state = state_1\n",
    "\n",
    "    print('successful episodes: {:d} - {:.4f}%'.format(successes, successes/episodes*100))\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c81968fe23e4946ac1acb2940f35da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441be208bace4d1b8baef1ff20bf665c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful episodes: 0 - 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a7faf891f442239dfb9456ba31c46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful episodes: 0 - 0.0000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 1000\n",
    "RUNS = 1\n",
    "\n",
    "q_data_pos = np.zeros((RUNS, EPISODES))\n",
    "dq_data_pos = np.zeros((RUNS, EPISODES))\n",
    "\n",
    "for i in tqdm_notebook(range(RUNS)):\n",
    "    position = DQN(EPISODES)\n",
    "    q_data_pos[i, :] = position\n",
    "\n",
    "    position = DoubleDQN(EPISODES)\n",
    "    dq_data_pos[i, :] = position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot percentage actions left over episodes:\n",
    "plt.plot(q_data_pos.mean(0), label='Q-learning')\n",
    "plt.plot(dq_data_pos.mean(0), label='Double Q-learning')\n",
    "plt.title('End position')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
